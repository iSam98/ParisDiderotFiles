{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2rydJ4wYJI9"
      },
      "source": [
        "## Loading and vectorizing texts with sklearn\n",
        "\n",
        "Scikit-learn has methods to transform a collection of documents into matrices of \"**bag of words**\" representations of these documents.\n",
        "\n",
        "These matrices use the scipy.sparse type, which is appropriate for **sparse matrices**.\n",
        "\n",
        "These modules have 3 methods:\n",
        "- fit : builds the vocabulary and the correspondance between word forms and word ids\n",
        "- transform : transforms the documents into matrices of counts\n",
        "- fit_transform : performs both actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJH_fHRDYJJE",
        "outputId": "ca29ca60-a91a-4938-eb80-bd0bac05b51c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type of X_train <class 'scipy.sparse.csr.csr_matrix'>\n",
            "shape of X_train (4, 15)\n",
            "  (0, 1)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 13)\t1\n",
            "  (0, 2)\t1\n",
            "  (1, 4)\t1\n",
            "  (1, 13)\t1\n",
            "  (1, 2)\t2\n",
            "  (1, 0)\t1\n",
            "  (1, 3)\t1\n",
            "  (1, 9)\t1\n",
            "  (2, 5)\t1\n",
            "  (2, 14)\t1\n",
            "  (2, 8)\t1\n",
            "  (2, 12)\t1\n",
            "  (3, 4)\t1\n",
            "  (3, 2)\t1\n",
            "  (3, 8)\t2\n",
            "  (3, 11)\t1\n",
            "  (3, 6)\t1\n",
            "  (3, 10)\t1\n",
            "  (3, 7)\t1\n",
            "[[0 1 1 0 1 0 0 0 0 0 0 0 0 1 0]\n",
            " [1 0 2 1 1 0 0 0 0 1 0 0 0 1 0]\n",
            " [0 0 0 0 0 1 0 0 1 0 0 0 1 0 1]\n",
            " [0 0 1 0 1 0 1 1 2 0 1 1 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# a French corpus (to see what is going on with diacritics)\n",
        "train_corpus = [\n",
        "     'Ceci est un document.',\n",
        "     'Ce document est encore un document à moi.',\n",
        "     'Et voilà le troisième.',\n",
        "     'Le premier document est-il le plus intéressant?',\n",
        " ]\n",
        "vectorizer = CountVectorizer()\n",
        "# the vectorizer is empty : this generates an error\n",
        "#print(vectorizer.vocabulary_)\n",
        "#print(vectorizer.get_feature_names())\n",
        "\n",
        "# we can fill it using the training set\n",
        "# and transform the training set into a matrix\n",
        "X_train = vectorizer.fit_transform(train_corpus)\n",
        "\n",
        "# the matrix is sparse\n",
        "print(\"type of X_train\", type(X_train))\n",
        "print(\"shape of X_train\", X_train.shape) # 4 documents/sentences and 15 unique words/types \n",
        "print(X_train)\n",
        "\n",
        "# here it is as a standard matrix\n",
        "print(X_train.toarray()) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fNCMYkzYJJL",
        "outputId": "080d0c02-b638-4e30-8f25-8b65fb2f420d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ceci': 1, 'est': 4, 'un': 13, 'document': 2, 'ce': 0, 'encore': 3, 'moi': 9, 'et': 5, 'voilà': 14, 'le': 8, 'troisième': 12, 'premier': 11, 'il': 6, 'plus': 10, 'intéressant': 7}\n",
            "['ce' 'ceci' 'document' 'encore' 'est' 'et' 'il' 'intéressant' 'le' 'moi'\n",
            " 'plus' 'premier' 'troisième' 'un' 'voilà']\n",
            "\n",
            " Size of vocabulary:  15\n"
          ]
        }
      ],
      "source": [
        "# here is the mapping between word forms and ids (our \"w2i\" in previous lab session) \n",
        "print(vectorizer.vocabulary_) \n",
        "# the list of word forms (our i2w) \n",
        "print(vectorizer.get_feature_names_out()) \n",
        "\n",
        "#QUESTIONS: \n",
        "# What is the size of the vocabulary \n",
        "print(\"\\n Size of vocabulary: \", len(vectorizer.vocabulary_))\n",
        "# What does the 3rd column of X.train.toarray() represent ? \n",
        "## 3rd document - sparse vector representation \n",
        "# What is printed when printing the sparse matrix ? \n",
        "## for each row which is a document in itself and the columns which stand for the entire vocabulary, \n",
        "## 1s in each row indicate that a token of that type(word) is present in the given document, while 0s indicate an absence (thus a unit vector?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4pX854QYJJN",
        "outputId": "fb7103df-f55c-475e-97cf-12d5c0851ba8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of X_test (2, 15)\n",
            "['ce' 'ceci' 'document' 'encore' 'est' 'et' 'il' 'intéressant' 'le' 'moi'\n",
            " 'plus' 'premier' 'troisième' 'un' 'voilà']\n",
            "[[0 0 1 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 1 1 1 1 1 0 0 0 0 0 0 0 1 0]]\n"
          ]
        }
      ],
      "source": [
        "test_corpus = [ 'Ah un nouveau document.',\n",
        "              'Et ceci est encore un document.']\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(train_corpus)\n",
        "X_test = vectorizer.transform(test_corpus)\n",
        "print(\"shape of X_test\", X_test.shape)\n",
        "\n",
        "# What happened to the words in test_corpus that are not present in train_corpus? \n",
        "print(vectorizer.get_feature_names_out()) \n",
        "print(X_test.toarray()) \n",
        "## the words have been assigned a 0 in the sparse matrix since they haven't been encountered in the train phase \n",
        "# Compare to vectorizer.fit_transform\n",
        "## if using fit_transform, the vocabulary will be extracted as if the test set is an input due to the presence of fit \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns-X-HZKYJJO",
        "outputId": "a5396bc6-28ba-4614-cc2f-c059f5966f20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MEMBERS:\n",
            " ('input', 'content')\n",
            "('encoding', 'utf-8')\n",
            "('decode_error', 'strict')\n",
            "('strip_accents', None)\n",
            "('preprocessor', None)\n",
            "('tokenizer', None)\n",
            "('analyzer', 'word')\n",
            "('lowercase', True)\n",
            "('token_pattern', '(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
            "('stop_words', None)\n",
            "('max_df', 1.0)\n",
            "('min_df', 1)\n",
            "('max_features', None)\n",
            "('ngram_range', (1, 1))\n",
            "('vocabulary', None)\n",
            "('binary', False)\n",
            "('dtype', <class 'numpy.int64'>)\n",
            "('fixed_vocabulary_', False)\n",
            "('_stop_words_id', 4347562040)\n",
            "('stop_words_', set())\n",
            "('vocabulary_', {'ceci': 1, 'est': 4, 'un': 13, 'document': 2, 'ce': 0, 'encore': 3, 'moi': 9, 'et': 5, 'voilà': 14, 'le': 8, 'troisième': 12, 'premier': 11, 'il': 6, 'plus': 10, 'intéressant': 7})\n"
          ]
        }
      ],
      "source": [
        "train_corpus = [\n",
        "     'Ceci est un document .',\n",
        "     'Ce document est encore un document à moi .',\n",
        "     'Et voilà le troisième .',\n",
        "     'Le premier document est -il le plus intéressant ?',\n",
        " ]\n",
        "\n",
        "# QUESTIONS: \n",
        "\n",
        "# How can you change the tokenization that the CountVectorizer will use (see its constructor)zer)\n",
        "# in particular, how to split on spaces only\n",
        "#  (which corresponds to supposing texts were already tokenized)\n",
        "# Indications: study \n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "# to see all the members of the instance, \n",
        "# and deduce which member to modify:\n",
        "print(\"\\nMEMBERS:\\n\", \"\\n\".join([ str(x) for x in vectorizer.__dict__.items()]))\n",
        "\n",
        "## modify the regular expression 'r' in 'token_pattern' to split on spaces alone \n",
        "\n",
        "# Which parameters can you modify to switch to bigram and trigram of characters features \n",
        "\n",
        "## analyzer can be changed to char and ngram_range can be changed to (2, 3)\n",
        "\n",
        "# Search what is a TF.IDF weighting (very famous) \n",
        "\n",
        "## used to reduce the impact of frequently occurring tokens \n",
        "\n",
        "# Study the TfidfVectorizer class\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "# dans deduce how to obtain TF.IDF weigthed vector representations of the documents\n",
        "\n",
        "## the class has an attribute idf_ that gives the weighted vector "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e9dOH4gYJJR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "sam_sklearn_extract_features_from_text.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}